%!TEX root = pixel-wise-street-segmentation.tex

\section{Used models}\label{sec:model}

\subsection{The Sliding Window Approach}

Traditionally Neuronal Networks are used for Classification tasks and as discredited in the introduction they deliver impressive results. Our first approach is a sliding window model, which exploits the classification strength of deep neuronal networks.

\subsubsection{Definition of the Classification Problem}

We trained a neuronal network to solve the following binary classification problem:

\fbox{
	\begin{tabular}{l l}
		\multicolumn{2}{l}{Classification Problem} \\
		\textit{Input:} & A $n \times n$ 3-channel pixel image section.\\
		\textit{Output:} & Decide whether the center pixel is street.
	\end{tabular}
} 

For $n$ we used $51$. This constant was choosen as we ran into gpu memory problems when training on higher values of $n$. Our classification approach is visualized in Fig. \ref{fig:figure}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\columnwidth]{figures/models/sliding_window.png}
	\caption{Visualisation of the classification problem solved by our neuronal network.}
	\label{fig:figure}
\end{figure} 

\subsubsection{Net Topology}


The problem defined above can be tackled with any of the well known classification Net such as GoogleNet or Alexnet. For our solution we design our own Network detailed in \Cref{tab:topo}. With three hidden layers our Network is rather small. Our experiments shows that this small networks performs better than networks with more layers. One reason for this is that bigger networks are more vulnerable to overfitting. Hence with our limited amount of distinct data a smaller net will generalize much better. Secondly the binary decision task of recognizing street is much simpler than detailed image classification. This simplicity will also reflect in the net topology.

\begin{table}[H]
	\normalsize
	\centering
\begin{tabular}{r l l}
	Layer & Type  & Shape  \\
	0     & Input &  $51 \times 51 \times 3$ \\
	1     & Convolution & 10 filter  each $5 \times 5$ \\
	2     & Convolution & 10 filter  each $5 \times 5$  \\
	3     & Pooling     & $2 \times 2$ \\
	4     & Output     & $1$ \\
\end{tabular}
\caption{Topology of our Classification Net}
\label{tab:topo}
\end{table} 

\subsubsection{Training}

The training data for this classification problem can be easily obtained by modifying the original training data. One advantage of our approach is, that we get a lot of training data out of each image. In theory we get one (distinct) datum for each pixel in each training image. However it is not useful to actually use all of this data. Pixel which are close to each other deliver a very similar input as their is a high intersect in the corresponding input section. Hence the information gain of including this inputs are very small. On the other hand if we generate an image section for each pixel we obtain more data than the memory of our gpu can handle. We therefore introduced a training stride. If we set the stride to $s$ the center pixel selected for data generation have a distance of $s$ in each dimension to its nearest neighbour. The overlap of two adjacent image is hence reduced to $n-s$, where $n \times n$ is the size of each image section. Empirical evaluations indicated that $s=10$ is a good default value for the trainings stride.



\subsubsection{Evaluation}

In order to apply a classification Net on the segmentation problem we used the well known sliding window approach. The main idea is to apply the classification net on each pixel p of the input image by generating the $n \times n$ image section with center pixel $p$. We use padding to be able to apply the method to pixels close to the border. \Cref{fig:stride2} shows the result of this approach.

\begin{figure}[H]
	\centering
	\includegraphics[width=\columnwidth]{figures/models/testing2-um_32_sliding_stride2.png}
	\caption{Using the sliding window approach with a stride of 2.}
	\label{fig:stride2}
\end{figure} 

The main disadvantage of this approach is the impractical runtime. For a 1 mega-pixel image we need to run 1 million classifications. This leads to a runtime of almost 2 minutes with our hardware. In order to reduce the evaluation time we introduced an evaluation stride s. Similar to the training stride we skip s-1 pixel in each dimension. This increases the evaluation speed by a factor of $s^2$. For the sliding window approach we found that a stride of $s = 10$ is a reasonable trade-off between speed and quality. \Cref{fig:stride10} shows the result of the sliding window approach with a stride of 10.



\begin{figure}[H]
	\centering
	\includegraphics[width=\columnwidth]{figures/models/testing2-um_32_sliding_stride10.png}
	\caption{Using the sliding window approach with a stride of 10.}
	\label{fig:stride10}
\end{figure} 


\subsection{The Regression Approach}

The main disadvantage of our sliding window approach is that the segmentation becomes very coarse with high stride. To overcome this problem we designed a regression neuronal networks which is able to classify each pixel independently.

\subsubsection{Definition of the Regression Problem}

We trained a neuronal network to solve the following regression problem:

\fbox{
	\begin{tabular}{l l}
		\multicolumn{2}{l}{Regression Problem} \\
		\textit{Input:} & A $n \times n$ 3-channel pixel image section.\\
		\textit{Output:} & A $n \times n$ label.
	\end{tabular}
} 

where the net is trained to minimize the mean squared error of the output. The output of the regression net is continuous. We round the output to obtain a binary classification.  \Cref{fig:reg} visualizes our regression approach.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\columnwidth]{figures/models/fully-conv.png}
	\caption{Visualisation of the regression approach.}
	\label{fig:reg}
\end{figure} 

Our goal is to choose $n$ as big as possible as $n^2$ is the number of pixel which can be classified at once. However due to GPU memory limitations we can not train a network with $n > 51$.





\subsubsection{Net Topology}

Similar to the classification approach our experiments show that a simple net topologies works best. The topology we used is detailed in \cref{tab:topo2}.

\begin{table}[H]
	\normalsize
	\centering
	\begin{tabular}{r l l}
		Layer & Type  & Shape  \\
		0     & Input &  $51 \times 51 \times 3$ \\
		1     & Convolution & 10 filter  each $5 \times 5$ \\
		2     & Convolution & 1 filter $51 \times 51$  \\
		3     & Reshape (Flatten) & $51 \times 51$ \\
		4     & Output     & $51^2 \times 1$ \\
	\end{tabular}
	\caption{Topology of our Regression Net}
	\label{tab:topo2}
\end{table} 

\subsubsection{Training}

Training of the regression model can be implemented analogously to the classification model. We use overlapping image-section again to get as much information out of the data as possible.

\subsubsection{Evaluation}

\begin{figure}[]
	\centering
	\includegraphics[width=\columnwidth]{figures/models/testing2-um_32_conv_stride51.png}
	\caption{Using regression approach with stride of 51}
	\label{fig:reg_stride2}
\end{figure} 



\begin{figure}[]
	\centering
	\includegraphics[width=\columnwidth]{figures/models/testing2-um_32_conv_stride37.png}
	\caption{Using regression approach with stride of 37}
	\label{fig:reg_stride10}
\end{figure} 

\subsection{Discussion}