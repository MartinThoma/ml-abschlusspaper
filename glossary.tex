%!TEX root = thesis.tex
%Term definitions
\newacronym{AEP}{AEP}{auto-encoder pretraining}
\newacronym{ANN}{ANN}{artificial neural network}
\newacronym{CE}{CE}{cross entropy}
\newacronym{CFM}{CFM}{classification figure of merit}
\newacronym{CNN}{CNN}{Convolutional Neural Network}
\newacronym{CUDA}{CUDA}{Compute Unified Device Architecture}
\newacronym{GMM}{GMM}{Gaussian mixture model}
\newacronym{GPU}{GPU}{graphics processing unit}
\newacronym{LDA}{LDA}{linear discriminant analysis}
\newacronym{MLP}{MLP}{multilayer perceptron}
\newacronym{MSE}{MSE}{mean squared error}
\newacronym{PCA}{PCA}{principal component analysis}
\newacronym{PyPI}{PyPI}{Python Package Index}
\newacronym{SLP}{SLP}{supervised layer-wise pretraining}
\newacronym{sst}{sst}{street segmentation toolkit}
\newacronym{SVM}{SVM}{support vector machine}


% Term definitions
\newglossaryentry{epoch}{name={epoch}, description={During iterative training of a neural network, an \textit{epoch} is a single pass through the entire training set, followed by testing of the verification set.\cite{Concise12}}}

\newglossaryentry{hypothesis}{
    name={hypothesis},
    description={The recognition results which a classifier returns is called a hypothesis. In other words, it is the \enquote{guess} of a classifier},
    plural=hypotheses
}

\newglossaryentry{reference}{
    name={reference},
    description={Labeled data is used to evaluate classifiers. Those labels are called references},
}

\newglossaryentry{YAML}{name={YAML}, description={YAML is a human-readable data format that can be used for configuration files}}
\newglossaryentry{MER}{name={MER}, description={An error measure which combines symbols to equivalence classes. It was introduced on \cpageref{merged-error-introduction}}}

\newglossaryentry{JSON}{name={JSON}, description={JSON, short for JavaScript Object Notation, is a language-independent data format that can be used to transmit data between a server and a client in web applications}}

\newglossaryentry{hyperparamter}{name={hyperparamter}, description={A
\textit{hyperparamter} is a parameter of a neural net, that cannot be learned,
but has to be chosen}, symbol={\ensuremath{\theta}}}

\newglossaryentry{learning rate}{name={learning rate}, description={A factor $0 \leq \eta \in \mdr$ that affects how fast new weights are learned. $\eta=0$ means that no new data is learned}, symbol={\ensuremath{\eta}}} % Andrew Ng: \alpha

\newglossaryentry{learning rate decay}{name={learning rate decay}, description={The learning rate decay $0 < \alpha \leq 1$ is used to adjust the learning rate. After each epoch the learning rate $\eta$ is updated to $\eta \gets \eta \times \alpha$}, symbol={\ensuremath{\eta}}}

\newglossaryentry{preactivation}{name={preactivation}, description={The preactivation of a neuron is the weighted sum of its input, before the activation function is applied}}
