%!TEX root = pixel-wise-street-segmentation.tex

\section{Conclusion}\label{sec:discussion}

The results presented in this paper were obtained over five months in a
practical course. We started with the Caffe framework and tinkered around with
a fork created by Jonathan Long. The model he provided could not be used on our
computers, because \SI{4}{\giga\byte} of GPU RAM were not enough to evaluate
the model. We tried to adjust the model model, but we failed due to the lack of
documentation, cryptic error messages and random crashes while training or
evaluating. This was the reason why we switched to Lasagne. Using this
framework, we noticed that we still need to try many different topologies. Our
first tries lead to bad classification accuracy and we are not aware of any
analytical way to determine a network topology for a given task. For this
reason, we developed the SST framework. This allows developers to quickly
train, test, and evaluate new network topologies. Although the framework got
its final flexible form in the last month of the practical course, we used it
to evaluate regression and classification models with several topologies. The
results are described in \cref{sec:evaluation}.

In standard scenes, the classification accuracy is impressive. The street gets
segmented very well in a runtime of well below  $\SI{0.5}{\second}$. However,
in some images the model does perform very badly. These are mainly images with
special situations such as an uncommon street colors or unusual lightning. We
believe that these problems can be easily eliminated by using more training
data. Another approach to get better results on the KITTI data set is to train
the model with different data and only use the KITTI training data for
fine-tuning.

One advantage of our models is that they are perfectly parallelizable. Each
image section can be evaluated independently. This can be advantageous in
practical applications. When using specialized hardware such as neuromorphic
chips it is possible to build hundreds of cores in a car. In such a case our
classification approach can yield outstanding results. Given enough training
data (e.g. 1.2~million images) using GoogLeNet or AlexNet can provide perfect
classification results.

Finally one can also improve the results with better hardware. For some of our
models the \gls{GPU} RAM was the limiting factor. Especially for the regression
model using a bigger section of the image can lead to much better results in
quality as well as in speed.
